{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45545cf7",
   "metadata": {},
   "source": [
    "1) Generate dim_date (2020–2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "915103ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dates=[]\n",
    "for y in range(2020, 2026):\n",
    "    for m in range(1,13):\n",
    "        dates.append({\"date_id\":y*100+m,\"year\":y,\"month\":m})\n",
    "dim_date = pd.DataFrame(dates)\n",
    "dim_date.to_csv(\"data_warehouse1/dim_date.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c503e83",
   "metadata": {},
   "source": [
    "**2) Process `passenger2020–2025` → produce two tables (`fact_route_monthly.csv` & `fact_passenger_monthly.csv`)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dc33c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ fact_route_monthly.csv generated\n",
      "✓ fact_passenger_monthly.csv generated\n",
      "(No dim_airport — processed exactly according to your requirement)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, glob, os\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. Read and merge passenger T-100 data\n",
    "# --------------------------------------------------\n",
    "files = glob.glob(\"passenger2020-2025/*.csv\")\n",
    "dfs = []\n",
    "for f in files:\n",
    "    df = pd.read_csv(f, low_memory=False)\n",
    "    df.columns = [c.lower().strip() for c in df.columns]\n",
    "    keep = [c for c in [\n",
    "        \"year\",\"month\",\"carrier\",\"origin\",\"origin_airport_id\",\n",
    "        \"dest\",\"dest_airport_id\",\"passengers\",\"seats\",\"distance\"\n",
    "    ] if c in df.columns]\n",
    "    df = df[keep]\n",
    "    dfs.append(df)\n",
    "\n",
    "raw = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Numeric cleaning\n",
    "for c in [\"year\",\"month\",\"seats\",\"origin_airport_id\",\"dest_airport_id\"]:\n",
    "    if c in raw:\n",
    "        raw[c] = pd.to_numeric(raw[c], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "for c in [\"passengers\",\"distance\"]:\n",
    "    if c in raw:\n",
    "        raw[c] = pd.to_numeric(raw[c], errors=\"coerce\")\n",
    "\n",
    "raw[\"origin\"] = raw[\"origin\"].str.upper().str.strip()\n",
    "raw[\"dest\"]   = raw[\"dest\"].str.upper().str.strip()\n",
    "\n",
    "raw = raw.dropna(subset=[\"year\",\"month\",\"origin\",\"dest\"])\n",
    "\n",
    "# ⭐⭐⭐ Only study AA ⭐⭐⭐\n",
    "raw = raw[raw[\"carrier\"] == \"AA\"]\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. fact_route_monthly (O-D × month)\n",
    "# --------------------------------------------------\n",
    "agg = {\"passengers\": \"sum\"}\n",
    "if \"seats\" in raw:     agg[\"seats\"] = \"sum\"\n",
    "if \"distance\" in raw:  agg[\"distance\"] = \"median\"\n",
    "\n",
    "fr = raw.groupby([\"year\",\"month\",\"origin\",\"dest\"], as_index=False).agg(agg)\n",
    "fr[\"date_id\"] = fr[\"year\"] * 100 + fr[\"month\"]\n",
    "\n",
    "# ⭐ No longer using dim_airport → export IATA directly ⭐\n",
    "fr = fr[[\"year\",\"month\",\"date_id\",\"origin\",\"dest\",\"passengers\"]\n",
    "        + ([\"seats\"] if \"seats\" in fr else [])\n",
    "        + ([\"distance\"] if \"distance\" in fr else [])]\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. fact_passenger_monthly (airport × month, in + out)\n",
    "# --------------------------------------------------\n",
    "outb = fr.groupby([\"date_id\",\"year\",\"month\",\"origin\"], as_index=False)[\"passengers\"].sum() \\\n",
    "         .rename(columns={\"origin\":\"airport\",\"passengers\":\"pax_out\"})\n",
    "\n",
    "inb = fr.groupby([\"date_id\",\"year\",\"month\",\"dest\"], as_index=False)[\"passengers\"].sum() \\\n",
    "         .rename(columns={\"dest\":\"airport\",\"passengers\":\"pax_in\"})\n",
    "\n",
    "# Outer join to merge\n",
    "fp = outb.merge(inb, on=[\"date_id\",\"year\",\"month\",\"airport\"], how=\"outer\").fillna(0)\n",
    "fp[\"pax\"] = fp[\"pax_out\"] + fp[\"pax_in\"]\n",
    "\n",
    "fp = fp[[\"year\",\"month\",\"date_id\",\"airport\",\"pax_out\",\"pax_in\",\"pax\"]]\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. Save (only fact tables, no dim tables)\n",
    "# --------------------------------------------------\n",
    "os.makedirs(\"data_warehouse1\", exist_ok=True)\n",
    "\n",
    "fr.to_csv(\"data_warehouse1/fact_route_monthly.csv\", index=False)\n",
    "fp.to_csv(\"data_warehouse1/fact_passenger_monthly.csv\", index=False)\n",
    "\n",
    "print(\"✓ fact_route_monthly.csv generated\")\n",
    "print(\"✓ fact_passenger_monthly.csv generated\")\n",
    "print(\"(No dim_airport — processed exactly according to your requirement)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5840f01a",
   "metadata": {},
   "source": [
    "**3. Process `airport_state.txt` (airport–state mapping) → `dim_airport_state.csv`**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25126562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Success! Number of parsed airports: 1484\n",
      "  airport state_abbr\n",
      "0     03A         AK\n",
      "1     04A         AK\n",
      "2     05A         AK\n",
      "3     06A         AK\n",
      "4     1AK         AK\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "records = []\n",
    "\n",
    "# Matching pattern: \", AL:\" and \"(ANB)\"\n",
    "pattern = re.compile(r\",\\s*([A-Z]{2}):.*\\(([A-Z0-9]{2,4})\\)$\")\n",
    "\n",
    "with open(\"airport_state.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        m = pattern.search(line)\n",
    "        if m:\n",
    "            state_abbr = m.group(1)   # AL / AK / CA ...\n",
    "            iata = m.group(2)         # ANB / LAX / ANC ...\n",
    "            records.append((iata, state_abbr))\n",
    "\n",
    "df = pd.DataFrame(records, columns=[\"airport\", \"state_abbr\"])\n",
    "df = df.drop_duplicates().sort_values(\"airport\").reset_index(drop=True)\n",
    "\n",
    "df.to_csv(\"data_warehouse1/dim_airport_state.csv\", index=False)\n",
    "print(\"✓ Success! Number of parsed airports:\", len(df))\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3667872",
   "metadata": {},
   "source": [
    "**4) Process delay causes (delaycause2020–2025) → `fact_delay_monthly.csv`**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67b3023a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Column Structure Check Results ===\n",
      "\n",
      "Columns: ('year', 'month', 'carrier', 'carrier_name', 'airport', 'airport_name', 'arr_flights', 'arr_del15', 'carrier_ct', 'weather_ct', 'nas_ct', 'security_ct', 'late_aircraft_ct', 'arr_cancelled', 'arr_diverted', 'arr_delay', 'carrier_delay', 'weather_delay', 'nas_delay', 'security_delay', 'late_aircraft_delay')\n",
      "Files: ['delaycause2020.1-2025.7\\\\Airline_Delay_Cause.csv']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, glob\n",
    "\n",
    "files = glob.glob(\"delaycause2020.1-2025.7/*.csv\")\n",
    "\n",
    "all_columns = []\n",
    "\n",
    "for f in files:\n",
    "    d = pd.read_csv(f, nrows=5)   # Read only the first few rows to improve speed\n",
    "    cols = tuple(d.columns.str.lower().str.strip())  # Normalize column names\n",
    "    all_columns.append((f, cols))\n",
    "\n",
    "# --- Check unique sets of column names ---\n",
    "unique_structures = {}\n",
    "\n",
    "for f, cols in all_columns:\n",
    "    unique_structures.setdefault(cols, []).append(f)\n",
    "\n",
    "# Output results\n",
    "print(\"\\n=== Column Structure Check Results ===\")\n",
    "for cols, files_with_these_cols in unique_structures.items():\n",
    "    print(\"\\nColumns:\", cols)\n",
    "    print(\"Files:\", files_with_these_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e02e5a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ fact_delay_monthly generated!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Part 1: Read delaycause & clean (your original code)\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Read CSV\n",
    "fd = pd.read_csv(\"delaycause2020.1-2025.7/Airline_Delay_Cause.csv\")\n",
    "\n",
    "# ⭐⭐⭐ Only study AA (the most important line) ⭐⭐⭐\n",
    "fd = fd[fd[\"carrier\"] == \"AA\"]\n",
    "\n",
    "# Convert column names to lowercase\n",
    "fd.columns = [c.lower().strip() for c in fd.columns]\n",
    "\n",
    "# Convert to numeric\n",
    "delay_cols = [\n",
    "    \"arr_delay\", \"carrier_delay\", \"weather_delay\",\n",
    "    \"nas_delay\", \"security_delay\", \"late_aircraft_delay\"\n",
    "]\n",
    "\n",
    "for c in delay_cols:\n",
    "    fd[c] = pd.to_numeric(fd[c], errors=\"coerce\")\n",
    "\n",
    "# Create date_id\n",
    "fd[\"date_id\"] = fd[\"year\"].astype(int) * 100 + fd[\"month\"].astype(int)\n",
    "\n",
    "# Airport code\n",
    "fd[\"airport\"] = fd[\"airport\"].str.upper().str.strip()\n",
    "\n",
    "drop_cols = [\"carrier_name\", \"airport_name\"]\n",
    "fd = fd.drop(columns=[c for c in drop_cols if c in fd.columns])\n",
    "\n",
    "# Create directory\n",
    "os.makedirs(\"data_warehouse1\", exist_ok=True)\n",
    "\n",
    "# Save fact_delay_monthly\n",
    "fd.to_csv(\"data_warehouse1/fact_delay_monthly.csv\", index=False)\n",
    "print(\"✓ fact_delay_monthly generated!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05da1151",
   "metadata": {},
   "source": [
    "**5) Process `stormevent2020–2025` → `fact_weather_state_monthly.csv` & `fact_weather_state_event_type_monthly.csv`**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b8ced48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Column Structure Check Results ===\n",
      "\n",
      "Columns: ('begin_yearmonth', 'begin_day', 'begin_time', 'end_yearmonth', 'end_day', 'end_time', 'episode_id', 'event_id', 'state', 'state_fips', 'year', 'month_name', 'event_type', 'cz_type', 'cz_fips', 'cz_name', 'wfo', 'begin_date_time', 'cz_timezone', 'end_date_time', 'injuries_direct', 'injuries_indirect', 'deaths_direct', 'deaths_indirect', 'damage_property', 'damage_crops', 'source', 'magnitude', 'magnitude_type', 'flood_cause', 'category', 'tor_f_scale', 'tor_length', 'tor_width', 'tor_other_wfo', 'tor_other_cz_state', 'tor_other_cz_fips', 'tor_other_cz_name', 'begin_range', 'begin_azimuth', 'begin_location', 'end_range', 'end_azimuth', 'end_location', 'begin_lat', 'begin_lon', 'end_lat', 'end_lon', 'episode_narrative', 'event_narrative', 'data_source')\n",
      "Files: ['stormevent2020-2025\\\\StormEvents_details-ftp_v1.0_d2020_c20250702.csv', 'stormevent2020-2025\\\\StormEvents_details-ftp_v1.0_d2021_c20250520.csv', 'stormevent2020-2025\\\\StormEvents_details-ftp_v1.0_d2022_c20250721.csv', 'stormevent2020-2025\\\\StormEvents_details-ftp_v1.0_d2023_c20250731.csv', 'stormevent2020-2025\\\\StormEvents_details-ftp_v1.0_d2024_c20250818.csv', 'stormevent2020-2025\\\\StormEvents_details-ftp_v1.0_d2025_c20250818.csv']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, glob\n",
    "\n",
    "files = glob.glob(\"stormevent2020-2025/*.csv\")\n",
    "\n",
    "all_columns = []\n",
    "\n",
    "for f in files:\n",
    "    d = pd.read_csv(f, nrows=5)   # Read only the first few rows to improve speed\n",
    "    cols = tuple(d.columns.str.lower().str.strip())  # Normalize column names\n",
    "    all_columns.append((f, cols))\n",
    "\n",
    "# --- Check unique column name combinations ---\n",
    "unique_structures = {}\n",
    "\n",
    "for f, cols in all_columns:\n",
    "    unique_structures.setdefault(cols, []).append(f)\n",
    "\n",
    "# Output results\n",
    "print(\"\\n=== Column Structure Check Results ===\")\n",
    "for cols, files_with_these_cols in unique_structures.items():\n",
    "    print(\"\\nColumns:\", cols)\n",
    "    print(\"Files:\", files_with_these_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94e62e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\14277\\AppData\\Local\\Temp\\ipykernel_31560\\2274165273.py:58: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"begin_ts\"] = pd.to_datetime(df[\"begin_date_time\"], errors=\"coerce\")\n",
      "C:\\Users\\14277\\AppData\\Local\\Temp\\ipykernel_31560\\2274165273.py:58: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"begin_ts\"] = pd.to_datetime(df[\"begin_date_time\"], errors=\"coerce\")\n",
      "C:\\Users\\14277\\AppData\\Local\\Temp\\ipykernel_31560\\2274165273.py:58: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"begin_ts\"] = pd.to_datetime(df[\"begin_date_time\"], errors=\"coerce\")\n",
      "C:\\Users\\14277\\AppData\\Local\\Temp\\ipykernel_31560\\2274165273.py:58: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"begin_ts\"] = pd.to_datetime(df[\"begin_date_time\"], errors=\"coerce\")\n",
      "C:\\Users\\14277\\AppData\\Local\\Temp\\ipykernel_31560\\2274165273.py:58: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"begin_ts\"] = pd.to_datetime(df[\"begin_date_time\"], errors=\"coerce\")\n",
      "C:\\Users\\14277\\AppData\\Local\\Temp\\ipykernel_31560\\2274165273.py:58: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"begin_ts\"] = pd.to_datetime(df[\"begin_date_time\"], errors=\"coerce\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ fact_weather_state_monthly.csv completed\n",
      "✓ fact_weather_state_event_type_monthly.csv completed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, glob, re, os\n",
    "\n",
    "# ---------- 1. State name → state abbreviation mapping ----------\n",
    "\n",
    "state_map = {\n",
    "    # 50 States\n",
    "    \"ALABAMA\": \"AL\", \"ALASKA\": \"AK\", \"ARIZONA\": \"AZ\", \"ARKANSAS\": \"AR\",\n",
    "    \"CALIFORNIA\": \"CA\", \"COLORADO\": \"CO\", \"CONNECTICUT\": \"CT\",\n",
    "    \"DELAWARE\": \"DE\", \"DISTRICT OF COL*\": \"DC\",\n",
    "    \"FLORIDA\": \"FL\", \"GEORGIA\": \"GA\", \"HAWAII\": \"HI\", \"IDAHO\": \"ID\",\n",
    "    \"ILLINOIS\": \"IL\", \"INDIANA\": \"IN\", \"IOWA\": \"IA\", \"KANSAS\": \"KS\",\n",
    "    \"KENTUCKY\": \"KY\", \"LOUISIANA\": \"LA\", \"MAINE\": \"ME\", \"MARYLAND\": \"MD\",\n",
    "    \"MASSACHUSETTS\": \"MA\", \"MICHIGAN\": \"MI\", \"MINNESOTA\": \"MN\",\n",
    "    \"MISSISSIPPI\": \"MS\", \"MISSOURI\": \"MO\", \"MONTANA\": \"MT\",\n",
    "    \"NEBRASKA\": \"NE\", \"NEVADA\": \"NV\", \"NEW HAMPSHIRE\": \"NH\",\n",
    "    \"NEW JERSEY\": \"NJ\", \"NEW MEXICO\": \"NM\", \"NEW YORK\": \"NY\",\n",
    "    \"NORTH CAROLINA\": \"NC\", \"NORTH DAKOTA\": \"ND\", \"OHIO\": \"OH\",\n",
    "    \"OKLAHOMA\": \"OK\", \"OREGON\": \"OR\", \"PENNSYLVANIA\": \"PA\",\n",
    "    \"RHODE ISLAND\": \"RI\", \"SOUTH CAROLINA\": \"SC\", \"SOUTH DAKOTA\": \"SD\",\n",
    "    \"TENNESSEE\": \"TN\", \"TEXAS\": \"TX\", \"UTAH\": \"UT\", \"VERMONT\": \"VT\",\n",
    "    \"VIRGINIA\": \"VA\", \"WASHINGTON\": \"WA\", \"WEST VIRGINIA\": \"WV\",\n",
    "    \"WISCONSIN\": \"WI\", \"WYOMING\": \"WY\",\n",
    "\n",
    "    # U.S. Territories used by NOAA StormEvents\n",
    "    \"PUERTO RICO\": \"PR\",\n",
    "    \"GUAM\": \"GU\",\n",
    "    \"VIRGIN ISLANDS\": \"VI\",\n",
    "    \"U.S. VIRGIN ISLANDS\": \"VI\",\n",
    "    \"AMERICAN SAMOA\": \"AS\",\n",
    "    \"NORTHERN MARIANA ISLANDS\": \"MP\",\n",
    "    \"MARSHALL ISLANDS\": \"MH\",\n",
    "    \"MICRONESIA\": \"FM\",\n",
    "    \"PALAU\": \"PW\",\n",
    "\n",
    "    # Special NOAA categories\n",
    "    \"ATLANTIC NORTH\": \"AN\",\n",
    "    \"ATLANTIC SOUTH\": \"ASO\",\n",
    "    \"GULF OF ALASKA\": \"GOA\",\n",
    "    \"LAKE HURON\": \"LH\",\n",
    "    \"LAKE MICHIGAN\": \"LM\",\n",
    "    \"LAKE SUPERIOR\": \"LS\",\n",
    "    \"LAKE ERIE\": \"LE\",\n",
    "    \"LAKE ONTARIO\": \"LO\",\n",
    "    \"PACIFIC NORTH\": \"PN\",\n",
    "    \"PACIFIC SOUTH\": \"PS\",\n",
    "    \"HAWAII WATERS\": \"HW\",\n",
    "    \"CARIBBEAN\": \"CB\"\n",
    "}\n",
    "\n",
    "files = glob.glob(\"stormevent2020-2025/*.csv\")\n",
    "dfs = []\n",
    "\n",
    "for f in files:\n",
    "    df = pd.read_csv(f, low_memory=False)\n",
    "    df.columns = [c.lower().strip() for c in df.columns]\n",
    "\n",
    "    # ---------- 2. Time ----------\n",
    "    df[\"begin_ts\"] = pd.to_datetime(df[\"begin_date_time\"], errors=\"coerce\")\n",
    "    df[\"year\"] = df[\"begin_ts\"].dt.year\n",
    "    df[\"month\"] = df[\"begin_ts\"].dt.month\n",
    "    df[\"date_id\"] = df[\"year\"] * 100 + df[\"month\"]\n",
    "\n",
    "    # ---------- 3. Standardize state names ----------\n",
    "    df[\"state_full\"] = df[\"state\"].str.upper().str.strip()\n",
    "    df[\"state_abbr\"] = df[\"state_full\"].map(state_map)\n",
    "\n",
    "    # Drop records whose states cannot be mapped (e.g., overseas territories)\n",
    "    df = df.dropna(subset=[\"state_abbr\"])\n",
    "\n",
    "    # ---------- 4. Convert damage amount ----------\n",
    "    def dmg(x):\n",
    "        if pd.isna(x):\n",
    "            return 0.0\n",
    "        s = str(x).strip().upper()\n",
    "        m = re.match(r\"([\\d\\.]+)\\s*([KMB])?\", s)\n",
    "        if not m:\n",
    "            return 0.0\n",
    "        val = float(m.group(1))\n",
    "        unit = m.group(2) or \"\"\n",
    "        return val * {\"K\": 1e3, \"M\": 1e6, \"B\": 1e9}.get(unit, 1)\n",
    "\n",
    "    df[\"damage\"] = df[\"damage_property\"].apply(dmg)\n",
    "\n",
    "    # ---------- 5. Severe event flag ----------\n",
    "    severe_set = {\n",
    "        \"THUNDERSTORM WIND\",\"HAIL\",\"BLIZZARD\",\"HEAVY RAIN\",\"FLASH FLOOD\",\n",
    "        \"FLOOD\",\"TORNADO\",\"HIGH WIND\",\"WINTER STORM\",\"ICE STORM\",\n",
    "        \"HURRICANE\",\"TROPICAL STORM\",\"WILDFIRE\"\n",
    "    }\n",
    "    df[\"event_type_clean\"] = df[\"event_type\"].str.upper().str.strip()\n",
    "    df[\"severe_flag\"] = df[\"event_type_clean\"].isin(severe_set).astype(int)\n",
    "\n",
    "    dfs.append(df[[\"state_abbr\", \"date_id\", \"event_id\",\n",
    "                   \"event_type_clean\", \"severe_flag\", \"damage\"]])\n",
    "\n",
    "wx = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# ---------- 6. Aggregate by state + month ----------\n",
    "fact_weather_state = wx.groupby(\n",
    "    [\"state_abbr\", \"date_id\"], as_index=False\n",
    ").agg(\n",
    "    events=(\"event_id\", \"count\"),\n",
    "    severe_events=(\"severe_flag\", \"sum\"),\n",
    "    total_damage=(\"damage\", \"sum\")\n",
    ")\n",
    "\n",
    "# ---------- 7. Normalized weather index ----------\n",
    "max_severe = fact_weather_state[\"severe_events\"].max()\n",
    "max_damage = fact_weather_state[\"total_damage\"].max()\n",
    "\n",
    "fact_weather_state[\"wx_index\"] = (\n",
    "    (fact_weather_state[\"severe_events\"] / (max_severe if max_severe > 0 else 1)) * 60 +\n",
    "    (np.log1p(fact_weather_state[\"total_damage\"]) /\n",
    "     (np.log1p(max_damage) if max_damage > 0 else 1)) * 40\n",
    ")\n",
    "\n",
    "os.makedirs(\"data_warehouse1\", exist_ok=True)\n",
    "fact_weather_state.to_csv(\"data_warehouse1/fact_weather_state_monthly.csv\",\n",
    "                          index=False)\n",
    "\n",
    "print(\"✓ fact_weather_state_monthly.csv completed\")\n",
    "\n",
    "# ---------- New: Aggregate by state + month + event type ----------\n",
    "fact_weather_state_type = wx.groupby(\n",
    "    [\"state_abbr\", \"date_id\", \"event_type_clean\"],\n",
    "    as_index=False\n",
    ").agg(\n",
    "    event_count=(\"event_id\", \"count\")\n",
    ")\n",
    "\n",
    "# Export\n",
    "fact_weather_state_type.to_csv(\n",
    "    \"data_warehouse1/fact_weather_state_event_type_monthly.csv\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"✓ fact_weather_state_event_type_monthly.csv completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e30de4f",
   "metadata": {},
   "source": [
    "**6) Process Gas/Fuel Prices → `fact_fuel_price_monthly.csv`**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "552e6d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Back to Contents  \\\n",
      "0             Sourcekey   \n",
      "1                  Date   \n",
      "2   1990-04-15 00:00:00   \n",
      "3   1990-05-15 00:00:00   \n",
      "4   1990-06-15 00:00:00   \n",
      "5   1990-07-15 00:00:00   \n",
      "6   1990-08-15 00:00:00   \n",
      "7   1990-09-15 00:00:00   \n",
      "8   1990-10-15 00:00:00   \n",
      "9   1990-11-15 00:00:00   \n",
      "10  1990-12-15 00:00:00   \n",
      "11  1991-01-15 00:00:00   \n",
      "12  1991-02-15 00:00:00   \n",
      "13  1991-03-15 00:00:00   \n",
      "14  1991-04-15 00:00:00   \n",
      "15  1991-05-15 00:00:00   \n",
      "16  1991-06-15 00:00:00   \n",
      "17  1991-07-15 00:00:00   \n",
      "18  1991-08-15 00:00:00   \n",
      "19  1991-09-15 00:00:00   \n",
      "\n",
      "   Data 1: U.S. Gulf Coast Kerosene-Type Jet Fuel Spot Price FOB (Dollars per Gallon)  \n",
      "0                                EER_EPJK_PF4_RGC_DPG                                  \n",
      "1   U.S. Gulf Coast Kerosene-Type Jet Fuel Spot Pr...                                  \n",
      "2                                                0.54                                  \n",
      "3                                               0.515                                  \n",
      "4                                               0.494                                  \n",
      "5                                               0.535                                  \n",
      "6                                               0.791                                  \n",
      "7                                               1.012                                  \n",
      "8                                               1.196                                  \n",
      "9                                               0.971                                  \n",
      "10                                              0.803                                  \n",
      "11                                              0.741                                  \n",
      "12                                              0.637                                  \n",
      "13                                              0.558                                  \n",
      "14                                              0.552                                  \n",
      "15                                              0.569                                  \n",
      "16                                              0.547                                  \n",
      "17                                              0.586                                  \n",
      "18                                              0.623                                  \n",
      "19                                              0.635                                  \n",
      "Columns: ['Back to Contents', 'Data 1: U.S. Gulf Coast Kerosene-Type Jet Fuel Spot Price FOB (Dollars per Gallon)']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_raw = pd.read_excel(\n",
    "    r\"./gas price2020-2025/EER_EPJK_PF4_RGC_DPGm.xls\",\n",
    "    sheet_name=\"Data 1\"\n",
    ")\n",
    "\n",
    "print(df_raw.head(20))\n",
    "print(\"Columns:\", df_raw.columns.tolist())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "896222d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ fact_fuel_price_monthly.csv generated\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "file = r\"./gas price2020-2025/EER_EPJK_PF4_RGC_DPGm.xls\"\n",
    "\n",
    "# Read the \"Data 1\" sheet\n",
    "df = pd.read_excel(file, sheet_name=\"Data 1\")\n",
    "\n",
    "# Standardize column names\n",
    "df.columns = [\"date_raw\", \"price_raw\"]\n",
    "\n",
    "# Drop the first two metadata rows (Sourcekey, Description)\n",
    "df = df.iloc[2:].reset_index(drop=True)\n",
    "\n",
    "# Parse datetime\n",
    "df[\"date\"] = pd.to_datetime(df[\"date_raw\"], errors=\"coerce\")\n",
    "\n",
    "# Drop empty rows\n",
    "df = df.dropna(subset=[\"date\"])\n",
    "\n",
    "# Extract year and month\n",
    "df[\"year\"] = df[\"date\"].dt.year\n",
    "df[\"month\"] = df[\"date\"].dt.month\n",
    "df[\"date_id\"] = df[\"year\"] * 100 + df[\"month\"]\n",
    "\n",
    "# ⭐⭐⭐ Keep only data from 202001 and later ⭐⭐⭐\n",
    "df = df[df[\"date_id\"] >= 202001]\n",
    "\n",
    "# Parse price\n",
    "df[\"price\"] = pd.to_numeric(df[\"price_raw\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows without price\n",
    "df = df.dropna(subset=[\"price\"])\n",
    "\n",
    "# Ensure monthly average (the dataset is already monthly)\n",
    "fact_fuel = df.groupby(\"date_id\", as_index=False)[\"price\"].mean()\n",
    "\n",
    "# Export to data warehouse\n",
    "os.makedirs(\"data_warehouse1\", exist_ok=True)\n",
    "fact_fuel.to_csv(\"data_warehouse1/fact_fuel_price_monthly.csv\", index=False)\n",
    "\n",
    "print(\"✓ fact_fuel_price_monthly.csv generated\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e23f61c",
   "metadata": {},
   "source": [
    "7.generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "041d68f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ dim_event_type.csv generated!\n",
      "   event_type_id       event_type_clean\n",
      "0              1  ASTRONOMICAL LOW TIDE\n",
      "1              2              AVALANCHE\n",
      "2              3               BLIZZARD\n",
      "3              4          COASTAL FLOOD\n",
      "4              5        COLD/WIND CHILL\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. Read the two fact files you provided\n",
    "# --------------------------------------------------\n",
    "\n",
    "fact_state = pd.read_csv(\"data_warehouse1/fact_weather_state_monthly.csv\")\n",
    "fact_evt = pd.read_csv(\"data_warehouse1/fact_weather_state_event_type_monthly.csv\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. Create dim_event_type (dim_weather_type)\n",
    "# --------------------------------------------------\n",
    "\n",
    "dim_event_type = (\n",
    "    pd.DataFrame(\n",
    "        sorted(fact_evt[\"event_type_clean\"].dropna().unique()),\n",
    "        columns=[\"event_type_clean\"]\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"event_type_id\"})\n",
    ")\n",
    "\n",
    "# Surrogate key starts from 1\n",
    "dim_event_type[\"event_type_id\"] = dim_event_type[\"event_type_id\"] + 1\n",
    "\n",
    "# Save the file\n",
    "dim_event_type.to_csv(\"data_warehouse1/dim_event_type.csv\", index=False)\n",
    "\n",
    "print(\"\\n✓ dim_event_type.csv generated!\")\n",
    "print(dim_event_type.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c31f8c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ dim_state.csv generated based on dim_airport_state\n",
      "   state_id state_abbr\n",
      "0         1         AK\n",
      "1         2         AL\n",
      "2         3         AR\n",
      "3         4         AZ\n",
      "4         5         CA\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Create dim_state — extracted from dim_airport_state\n",
    "# --------------------------------------------------\n",
    "\n",
    "aps = pd.read_csv(\"data_warehouse1/dim_airport_state.csv\")\n",
    "\n",
    "dim_state = (\n",
    "    pd.DataFrame(\n",
    "        sorted(aps[\"state_abbr\"].dropna().unique()),\n",
    "        columns=[\"state_abbr\"]\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"state_id\"})\n",
    ")\n",
    "\n",
    "# Surrogate key starts from 1\n",
    "dim_state[\"state_id\"] = dim_state[\"state_id\"] + 1\n",
    "\n",
    "# Save file\n",
    "dim_state.to_csv(\"data_warehouse1/dim_state.csv\", index=False)\n",
    "\n",
    "print(\"✓ dim_state.csv generated based on dim_airport_state\")\n",
    "print(dim_state.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
